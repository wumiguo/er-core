import java.util.Calendar

import org.apache.log4j.{FileAppender, Level, LogManager, SimpleLayout}
import org.apache.spark.{SparkConf, SparkContext}
import org.wumiguo.ser.dataloader.JSONWrapper
import org.wumiguo.ser.methods.datastructure.WeightedEdge
import org.wumiguo.ser.methods.entityclustering.{ConnectedComponentsClustering, EntityClusterUtils}
import org.wumiguo.ser.methods.similarityjoins.common.CommonFunctions
import org.wumiguo.ser.methods.similarityjoins.simjoin.PPJoin

val conf = new SparkConf()
  .setAppName("Main")
  .setMaster("local[*]")
  .set("spark.default.parallelism", "4")
  .set("spark.local.dir", "/data2/tmp")

val sc = new SparkContext(conf)


def performTest(dataset: String, basePath: String, workers: String): Unit = {
  val filePath = basePath + dataset + "profiles.json"
  val gtPath = basePath + dataset + "IdDuplicates.json"

  val logPath = "/gpfs/work/uMR19_dbgroup/log_ppjoin_" + dataset + "_workers_" + workers + ".txt"

  val log = LogManager.getRootLogger
  log.setLevel(Level.INFO)
  val layout = new SimpleLayout()
  val appender = new FileAppender(layout, logPath, false)
  log.addAppender(appender)

  /** Loads the profiles */
  val profiles = JSONWrapper.loadProfiles(filePath, realIDField = "realProfileID")

  /** Extract all the values */
  val docs = CommonFunctions.extractAllFields(profiles)
  docs.cache()
  val nd = docs.count()
  log.info("[PPJoin] Number of docs " + nd)

  /** Performs the similarity join */
  log.info("[PPJoin] Threshold 0.4")

  val t1 = Calendar.getInstance().getTimeInMillis
  val matches = PPJoin.getMatches(docs, 0.4)
  matches.cache()
  val nm = matches.count()
  docs.unpersist()

  val t2 = Calendar.getInstance().getTimeInMillis

  log.info("[PPJoin] Join+verification time (s) " + (t2 - t1) / 1000.0)
  log.info("[PPJoin] Number of matches " + nm)


  /** Performs the connected components */
  val clusters = ConnectedComponentsClustering.getClusters(profiles, matches.map(x => WeightedEdge(x._1, x._2, 0)), 0)
  clusters.cache()
  val cn = clusters.count()
  val t3 = Calendar.getInstance().getTimeInMillis
  log.info("[PPJoin] Number of clusters " + cn)
  log.info("[PPJoin] Clustering time (s) " + (t3 - t2) / 1000.0)

  log.info("[PPJoin] Total time (s) " + (t3 - t1) / 1000.0)

  /** Loads the groundtruth */
  val groundtruth = JSONWrapper.loadGroundtruth(gtPath, firstDatasetAttribute = "d1Id", secondDatasetAttribute = "d2Id")

  //Converts the ids in the groundtruth to the autogenerated ones
  val realIdIds = sc.broadcast(profiles.map { p =>
    (p.originalID, p.id)
  }.collectAsMap())

  var newGT: Set[(Int, Int)] = null
  newGT = groundtruth.map { g =>
    val first = realIdIds.value.get(g.firstEntityID)
    val second = realIdIds.value.get(g.secondEntityID)
    if (first.isDefined && second.isDefined) {
      val f = first.get
      val s = second.get
      if (f < s) (f, s) else (s, f)
    }
    else {
      (-1, -1)
    }
  }.filter(_._1 >= 0).collect().toSet


  log.info("[PPJoin] Groundtruth size " + groundtruth.count())
  log.info("[PPJoin] New groundtruth size " + newGT.size)

  val gt = sc.broadcast(newGT)


  /** Computes precision and recall */

  val pcpq = EntityClusterUtils.calcPcPqCluster(clusters, gt)
  log.info("[PPJoin] PC " + pcpq._1)
  log.info("[PPJoin] PQ " + pcpq._2)

  val f1 = 2 * ((pcpq._1 * pcpq._2) / (pcpq._1 + pcpq._2))
  log.info("[PPJoin] F1 " + f1)

  log.removeAllAppenders()
  sc.getPersistentRDDs.foreach(_._2.unpersist())
}



val numWorkers = sc.getConf.get("spark.driver.args").replace("\"", "")

val datasets = List("300K", "1M", "2M")

val basePath = "/gpfs/work/uMR19_dbgroup/ER/syntheticDatasets/"


datasets.foreach { dataset =>
  performTest(dataset, basePath, numWorkers)
}